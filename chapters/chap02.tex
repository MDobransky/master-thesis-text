\chapter{Neural Network Detectors}
\todo{bla}
NNs vs ML
define (NNs) abbr.

\section{Precision Metrics}
Before we describe any models, we need to define a metrics in order to be able to compare their performance. There are multiple criteria we are interested in, the main concern is accuracy and number of input images that can be processed per second (FPS). Because we are interested in real-time detector we need to find a balanced model.

There are multiple problems with state-of-the-art models presented by research groups. Many classification networks are designed for \textit{ImageNet} image recognition challenge\footnote{\url{http://www.image-net.org/challenges/LSVRC/}} that is evaluated purely by accuracy. Although detection networks use FPS as their main metrics, they often omit important details on how they calculated their precision, such as detection threasholds.

In this section we define commonly used metrics to evaluate a performance of neural networks and other machine learning approaches. Later we will use those metrics to compare previewed networks and our own implementations.

\subsection{Image recognition}
top-1, top-5 score

\subsection{Bounding box localisation}

precision-recal curve 
average precision
mean AP
IoU

\subsection{Speed}

\section{Classification networks}
\label{chapt:cnets}
In order to better understand detection networks, we will shortly describe their predecessors, classification networks. Classification network is a convolutional neural network (CNN) \cite[ch.~9]{bib:dlbook} that given an image, returns a confidence score of correspondence to each of the classes. Usually, \textit{soft-max} function is applied to the confidence score to represent a probability distribution. The \textit{ImageNet} image recognition challenge is usually used to benchmark the accuracy of such networks.

Later, in \cref{chapt:models}, we will present how classification models are used as a backbone for a detector network.

\subsection{AlexNet (2012)}
A significant breakthrough in use of CNNs happened in 2012, when the \textit{AlexNet} \cite{bib:alexnet} won the \textit{ImageNet} image recognition challenge. It was the first time a deep CNN performed better than traditional computer vision and machine learning approaches. 

AlexNet has a simple architecture with five convolutional layers and two fully connected layers, followed by a softmax layer. It has created a foundation on which today's state-of-the-art models are built and set a new standard for image recognition.

\subsection{VGG (2014)}
\label{sec:VGG}
The network architecture, mostly known as VGG \cite{bib:vgg}, pushed the concept of AlexNet even further and has proved the feasibility of deep network with small convolutions. 

Each of the VGG's convolutional filters uses 3$\times$3 kernel, and the depth of the filters is increased through the network, reaching 512 filters in the last layers. Three fully connected layers and softmax are applied after the convolutions, see \cref{tab:vggarch}. There are multiple versions of the VGG architecture, depending on the number of convolutional layers, the most popular is the 16 layer version. 

VGG network is considered a general architecture for a classification network due to its linear architecture with decreasing size of the features and increasing number of channels. 

\begin{table}
    \centering
    \rotatebox{90}{
        \vggArch
    }
    \caption{Architecture of VGG network, version D. Taken from \cite[table 1]{bib:vgg}}
    \label{tab:vggarch}
\end{table}
    
\subsection{Inception (v1) (2014)}
\label{sec:inception}
Previous architectures showed that increasing the number of layers and layer size, leads to better precision. Inception v1 \cite{bib:googlenet}, also known as GoogLeNet, aims at increasing precision while improving utilisation of computing resources.

To avoid the growing cost of stacking more layers, the network introduces the concept of sparsity by using the \textit{inception modules}, \cref{fig:incept_mod}. A sparse structure is approximated by using multiple convolutions with different kernel sizes and concatenating the outputs together. Max-pooling is also performed as an alternative to convolutions and concatenated to the output. Because high dimensional convolutions are costly, a reduction to the channels is introduced by using 1$\times$1 convolution as a preceding layer.

The network is then formed by linearly stacking nine inception modules, preceded by a linear stem network and followed by a fully connected classifier. Two auxiliary classifiers are added to intermediate layers of the network to help propagate gradients and provide regularisation during the training.

A set of improvements to the Inception network was introduced in later versions of the network. Most notably a factorisation of convolution layers was introduced in Inception v2 and v3 \cite{bib:inception2}.


\begin{figure}
    \includegraphics[width=\textwidth]{img/inception}
    \caption{Inception module, picture from \cite[figure 2]{bib:googlenet}.}
    \label{fig:incept_mod}
\end{figure}

\subsection{ResNet (2015)}
\label{sec:resnet}

Although it has become possible to train deeper and deeper CNNs, degradation has eventually been observed. Adding more layers to models started to produce higher training error. Theoretically, adding more layers to a smaller model should produce at least equal results, as the smaller model is the subspace of the larger one. A solution to this problem was proposed in the ResNet architecture, by directly introducing identity functions to the network \cite{bib:resnet}.  

A baseline of ResNet is directly inspired by VGG (\cref{sec:VGG}). Most of the convolutional layers have 3$\times$3 filters and follow two simple rules: keep the number of filters the same, unless changing the output size and double the filters if the feature size if halved. A residual connection is then added to each pair of the convolutional layers. This connection is either an identity or a projection done by 1$\times$1 convolution to match the increased number of filters. 

\begin{figure}
    \resnetArch
    \caption{Architecture of the ResNet network and residual blocks. Each of the four \textit{Layers} are created by stacking multiple residual blocks.}
    \label{fig:resnet_arch}
\end{figure}

We can see the high level architecture of this model in \cref{fig:resnet_arch} (left). Each of four \textit{Layers} is composed of multiple linearly stacked residual blocks, exact numbers of blocks can be found in \cite[table 1]{bib:resnet}. A feature map size is preserved inside each of the \textit{Layers} and halved between them. A type of residual block we described previously was a \textit{Basic block} with two convolutional layers and is used for smaller ResNet models (ResNet-18, ResNet-34). Deeper ResNet models (ResNet-50, ResNet-101, ResNet-152) use the \textit{Bottleneck block} with three convolutional layers, where the 1$\times$1 layers are responsible for reducing and then restoring dimensions, leaving the 3$\times$3 layer a with smaller input and output dimensions. Remarkably, the 152-layer ResNet has lower complexity than the 16-layer VGG network.



\subsection{Xception (2017)}
\label{sec:xception}
Xception architecture \cite{bib:xception} is heavily inspired by previous architectures, mainly \textit{Inception} and \textit{ResNet}. It is built on the hypothesis that "the mapping of cross-channel correlations and spatial correlations in the feature maps of convolutional neural networks can be
entirely decoupled". This hypothesis expands upon the hypothesis underlying \textit{Inception} architecture. Therefore the name Extreme Inception. 

The hypothesis is realised in the form of depthwise separable convolution layers. Depthwise separable convolution consists of two steps: a \textit{depthwise convolution} and \textit{pointwise convolution}. A depthwise convolution is a convolution performed independently over each channel, i.e. a convolution without changing the number of channels. The second step is a pointwise convolution that uses 1$\times$1 kernel to map the input channels into new channel space.

The architecture is created by linearly stacking convolutional layers with the addition of residual connections as seen on \cref{fig:xception}. A convolutional layers, non-linearity and poolings are structured into residual blocks similarly to \textit{ResNet} architecture.

\begin{figure}
    \includegraphics[width=\textwidth]{img/xception}
    \caption{Structure of \textit{Xception} architecture. Taken from \cite[fig. 5]{bib:xception}.}
    \label{fig:xception}
\end{figure}


\subsection{NASNet (2017)}
\label{sec:nasnet}
The difference between this architecture and the others mentioned is that \textit{NASNet} \cite{bib:nasnet} was designed by a machine learning algorithm. It is a result of a \textit{AutoML}\footnote{https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html} project that automates the design of network architecture. Unlike manually designing the network by trial and error, \textit{AutoML} searches space of all possible models, e.g. using reinforcement learning and evolutionary algorithms. The downside of this approach is the computational cost and therefore limitation to small datasets.

NASNet is a result of taking an architecture designed for small dataset (\textit{CIFAR-10}\footnote{https://www.cs.toronto.edu/~kriz/cifar.html}) by \textit{AutoML} and using it to create larger model for \textit{ImageNet} dataset. The model is composed of two types of learned cells, a \textit{Normal Cell} and \textit{Reduction Cell} (see \cref{fig:nasnet}). A general structure of the network is then created by alternating a \textit{Reduction Cell} and \textbf{N} \textit{Normal Cells}

\begin{figure}
    \includegraphics[width=\textwidth]{img/nasnet}
    \caption{Layers used in \textit{NASNet-A}, designed by \textit{AutoML}. Image from \url{ai.googleblog.com/2017/11/automl-for-large-scale-image}.}
    \label{fig:nasnet}
\end{figure}

\subsection{Comparing the classifiers}

\begin{figure}
    \includegraphics[width=\textwidth]{img/fps_comp}
    \caption{Taken from \cite[fig. 3]{bib:cnnbenchmark}}
    \label{fig:cnnbenchmark}
\end{figure}

\todo{add comp table for all models}

\section{Detection networks}
\label{chapt:models}

\todo{bla}

\subsection*{Region-based Convolutional Network (R-CNN)}
\subsubsection{R-CNN}
\cite{bib:rcnn}

\subsubsection{Fast R-CNN}

\subsubsection{Faster R-CNN}

\subsection*{Mask Region-based Convolutional Network (Mask R-CNN)}

\subsection*{You Only Look Once (YOLO)}
\subsubsection{YOLO}
\label{sec:yolo}
\subsubsection{YOLO v2 and YOLO 9000}

\subsection*{Single-Shot Detector (SSD)}
\label{sec:ssd}
\subsubsection{SSD}
\subsubsection{FSSD}
\subsubsection{RFB}


\section{Use of Detection networks for video processing}

%Optional section
% \section{Other uses of CNNs}
% \subsection{Noise removal/ Regularization}
% \subsection{Image generation}





